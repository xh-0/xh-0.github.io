---
title: 数据预处理
createTime: 2024/06/19 15:42:15
permalink: /dataMining/mgkiz6mo/
---
在工作中我们有一句口头禅："啊，这纷扰的世界!"。因为客户在 ==生产环境中采集到的数据通常无法直接用于数据挖掘==，数据中有无数的"坑"等着你去踩。而且每个算法对数据的要求都有不同的偏好，在数据挖掘之前我们首先需要考虑应当采取哪些预处理步骤来让数据更适合挖掘。当然，预处理是一个非常广泛的领域，我们在此节仅仅讨论其中一部分重要的，常用的思想。
<!-- more -->

## 聚集
聚集（aggregation）就是将两个或者多个对象合并成一个对象。例如一个商店在一天内产生了 100 个消费记录，对这些数据进行聚集的其中一个方法是：用品类的事务代替该商店的所有事务。此时单日的 100 个事务规约成了一个事务，而数据对象个数则从 100 降低为所有商品的品类数量。

聚集的目的：
1. 减少内存。聚集后数据集的对象数量变少，因此可以使用内存开销更大的数据挖掘算法。
2. 范围或标度的转换。
3. 对象或属性群的行为通常比单个个体的行为更加稳定。

::: warning 聚集的缺点
会丢失数据中的细节。
:::

## 抽样
有效抽样的原理就是：如果样本是具有代表性的，则使用样本和使用整个数据集的效果几乎一样。

抽样的目的：处理所有数据所需的内存或事件方面的计算成本太高。

### 抽样方法
#### 简单随机抽样（simple random sampling）
该抽样方式选取任何特定项的概率都相等。和其他所有抽样方式一样，它包含两个变式：
1. 无放回抽样。
2. 有放回抽样。
当样本于数据集相比相对较小时，这两种抽样方式区别不大。

#### 分层抽样（stratified sampling）
当总体由不同类型的对象组成并且每种类型的对象数量差别很大时，我们会用到分层抽样。

### 渐进抽样
由于可能很难确定合适的样本容量，因此有时候需要采取渐进抽样，即从一个小样本开始，主键增加样本数量直至得到足够容量的样本。

## 维归约
维规约的目的：
1. 提升算法表现。在较少的维度下大部分的挖掘算法效果会更好，因为删除了不相关的特征并降低了噪声。
2. 模型更容易理解。在过高的维度下，人类很难理解。
3. 维灾难。

### 维灾难
该现象是指：随着数据维度的增加，许多数据分析变得非常困难，数据在它所占据的空间中越来越稀疏。

### 维归约的线性代数技术
将数据从高维空间投影到低维空间，特别是对于连续数据。主成分分析（PCA）是一种用于连续属性的线性代数技术，它找出新的属性(主成分)，这些属性是原属性的线性组合，是相互 **正交的(orthogonal)**，并且捕获了数据的最大变差。

## 特征子集选择
除了像维归约一样创建出新的属性，还有更简单的降低维度的方法就是使用特征的一个子集进行数据挖掘。除了人工的先验知识的筛选，还有三种标准的特征选择方式：
- 嵌入方法（embedded approach）。

	将特征选择作为数据挖掘算法的一部分。
- 过滤方法（filter approach）。

	使用某种独立于数据挖掘任务的方法，在数据挖掘算法之前进行特征过滤。
- 包装方法（wrapper approach）。

	将目标数据挖掘算法作为黑盒，但是通常不枚举所有可能的子集来找出最佳属性子集。

::: tip 从我做的项目经验上来说，选取特征的过程和客户实际的业务是强关联的。选择特征集的过程一定要从小到大，即一开始只选择业务逻辑最相关的极少数特征，然后根据挖掘算法的结果逐步增加特征的数量。如果一开始就使用了过多的特征，一是给算法带来极大的空间和时间的压力，二来是后续优化时很难判断该删除哪个特征。
:::

## 特征创建
1. 特征提取。
2. 映射数据到新的空间。比如对时间序列实施傅里叶变换。

## 离散化和二元化
这个数据预处理方法可以说是最常用的两种方法，其本质的目的就是减少值的种类来令特定的种类变得频繁。

### 二元化
将 $m$ 个分类值衍生出 $m-1$ 个二元特征。

| 分类值 | 整数值 | $x_1$ | $x_2$ | $x_3$ |
| ------ | ------ | ----- | ----- | ----- |
| awful  | 0      | 0     | 0     | 0     |
| poor   | 1      | 0     | 0     | 1     |
| good   | 2      | 0     | 1     | 0     |
| great  | 3      | 0     | 1     | 1     |
::: center
一个分类属性到 3 个二元属性的变换。
:::

很显然，这种变换可能导致问题的复杂化，==可能建立起属性之间原本没有的联系==。比如 $x_2$ 和 $x_3$ 是相关的，因为 greate 是用这两个属性来表示的。因此对于关联问题，我们更倾向于非对称的二元变换：

| 分类值 | 整数值 | $x_1$ | $x_2$ | $x_3$ | $x_4$ |
| ------ | ------ | ----- | ----- | ----- | ----- |
| awful  | 0      | 1     | 0     | 0     | 0     |
| poor   | 1      | 0     | 1     | 1     | 0     |
| good   | 2      | 0     | 0     | 1     | 0     |
| great  | 3      | 0     | 0     | 0     | 1     |
::: center
一个分类属性到四个非对称二元属性的变换
:::

### 连续属性离散化
将连续属性排序后，通过指定 $n-1$ 个分割点（split point）将其分割成 $n$ 个区间。

#### 无监督离散化
即不使用类信息的离散化，通常是一些相对简单的方法：等宽，等频率。

#### 监督离散化
基于熵的方法是最有前途的离散化方法之一。

::: card  title="熵"
设 $k$ 是不同的类标号数，$m_i$ 是某划分的第 $i$ 个区间中值的个数，而 $m_{ij}$ 是区间 $i$ 中类 $j$ 的值的个数。第 $i$ 个区间的熵 $e_i$ 用如下公式表示：
$$
\tag{1} e_i = - \sum_{j=1}^k p_{ij} \log_2p_{ij}
$$
其中 $p_{ij} = \frac{m_{ij}}{m_i}$ 是第 $i$ 个区间中类 $j$ 的概率。该划分的总熵$e$是每个区间的熵的加权平均，即

$$
\tag{2} e=\sum_{i-1}^n \omega_i e_i
$$

其中，$m$ 是值的个数，$\omega_i = \frac{m_i}{m}$ 是第 $i$ 个区间的值的比例，而 $n$ 是区间个数。
:::

直观上来说，区间的熵是区间的纯度的度量。如果一个区间只包含一种值，则熵为 0，表示该区间非常纯，但是不影响总熵。如果一个区间中的值类出现的频率相等，即最不纯，则熵最大。

## 变量变换
### 简单函数
例如：$x^k$，$\log x$，$e^x$，$\frac{1}{x}$ 等。使用变量转换时需要谨慎，因为 ==改变了数据的特征== 。但是这种改变有时候又是必要的。

### 规范化或者标准化
目标时使得整个值的集合具有特定的性质。如果要以某种方法组合不同的变量，需要进行标准化来避免具有较大值域的变量左右分析结果。

例如：人的年龄之差通常比收入之差要小得多，如果不做标准化则会让收入之差极大地影响算法的结果。

*[PCA]: Principal Component Analysis